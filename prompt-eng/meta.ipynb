{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Prompting\n",
    "\n",
    "Meta prompting is an advanced technique in prompt engineering that emphasizes the structural and syntactical organization of tasks and problems rather than focusing on their specific content. The objective is to create a more abstract, form-driven way of engaging with large language models (LLMs), highlighting patterns and structure over traditional content-focused methods.\n",
    "\n",
    "As outlined by [Zhang et al. (2024)](https://arxiv.org/abs/2311.11482), the defining features of meta prompting include:\n",
    "\n",
    "* Structure-Oriented: Prioritizes the organization and pattern of problems and solutions instead of specific content.\n",
    "* Syntax-Guided: Leverages syntax as a template to shape the expected responses or solutions.\n",
    "* Abstract Frameworks: Uses abstract examples as blueprints, demonstrating the structure of tasks without relying on concrete details.\n",
    "* Domain Versatility: Can be applied across multiple fields, offering structured solutions to diverse problem types.\n",
    "* Categorical Approach: Draws on type theory to organize and categorize components logically, enhancing prompt coherence and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fmeta.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate this, we need to know that the logarithm of 2 (base 10) is approximately 0.301.\n",
      "\n",
      "So,\n",
      "\n",
      "984 * log(2) ≈ 984 * 0.301\n",
      "≈ 295.584\n",
      "Time taken: 5.399s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## META PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = \"What is 984 * log(2)\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "\n",
    "# @TODO TO BE COMPLETED\n",
    "PROMPT = MESSAGE \n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=1.0, \n",
    "                         num_ctx=100, \n",
    "                         num_predict=100)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': '\\nYou are an expert in prompt engineering. Given the following user request, generate a well-structured and detailed prompt that an AI model can use to generate a complete requirement analysis.\\n\\nUser Request:\\n\\nGenerate a prompt to generate requirement analysis for a Discord chatbot that:\\n- Enables data scientists and business analysts to efficiently locate and extract relevant data from company databases to answer specific queries and generate reports.\\n\\n', 'stream': False, 'options': {'temperature': 1.0, 'num_ctx': 500, 'num_predict': 500}}\n",
      "Generated AI Prompt for Requirement Analysis: Here's a well-structured and detailed prompt for an AI model to generate a complete requirement analysis for the Discord chatbot:\n",
      "\n",
      "**Prompt:**\n",
      "\n",
      "\"Design a comprehensive requirement analysis for a Discord-based chatbot aimed at facilitating data scientists and business analysts in extracting relevant data from company databases. The chatbot should enable users to efficiently locate and extract specific data, answer queries, and generate reports.\n",
      "\n",
      "**Requirements Analysis:**\n",
      "\n",
      "1. **User Profile and Authentication**\n",
      "\t* Provide an option for users to log in using their Discord credentials or create a new account.\n",
      "\t* Implement role-based access control (RBAC) to ensure only authorized personnel have access to sensitive data.\n",
      "2. **Data Retrieval and Filtering**\n",
      "\t* Integrate with company databases to retrieve relevant data based on user queries.\n",
      "\t* Allow users to filter data by various criteria, such as date ranges, data types, and fields of interest.\n",
      "3. **Query Language and Syntax**\n",
      "\t* Develop a simple query language (e.g., SQL-like syntax) that allows users to formulate specific queries.\n",
      "\t* Provide auto-suggestions or suggestions based on commonly used queries to facilitate user input.\n",
      "4. **Data Visualization and Report Generation**\n",
      "\t* Generate visualizations (e.g., charts, tables, graphs) to help users understand and present data insights.\n",
      "\t* Offer options for customizing report templates and formatting.\n",
      "5. **Error Handling and Feedback Mechanism**\n",
      "\t* Implement a feedback loop to notify users of errors or inconsistencies in their queries.\n",
      "\t* Provide clear instructions and suggestions for resolving query-related issues.\n",
      "6. **Security and Data Encryption**\n",
      "\t* Ensure all data transmitted between the chatbot and company databases is encrypted (e.g., SSL/TLS).\n",
      "\t* Store sensitive data securely, with access controls and audit logs to monitor user activity.\n",
      "7. **User Interface and Experience**\n",
      "\t* Design an intuitive interface that accommodates users' varied preferences and expertise levels.\n",
      "\t* Incorporate clear instructions, tooltips, and help resources for users to navigate the chatbot's features.\n",
      "8. **Scalability and Performance**\n",
      "\t* Optimize the chatbot's performance for large datasets and high user traffic.\n",
      "\t* Regularly update software components to maintain system reliability and security.\n",
      "\n",
      "Please expand on these requirements to include more specific details and technical specifications, such as:\n",
      "\n",
      "* Database management systems (e.g., relational databases like MySQL, PostgreSQL; NoSQL databases like MongoDB)\n",
      "* Frontend technologies (e.g., web frameworks\n",
      "Meta prompting processing time: 9.53s\n",
      "{'model': 'llama3.2:latest', 'prompt': 'Here\\'s a well-structured and detailed prompt for an AI model to generate a complete requirement analysis for the Discord chatbot:\\n\\n**Prompt:**\\n\\n\"Design a comprehensive requirement analysis for a Discord-based chatbot aimed at facilitating data scientists and business analysts in extracting relevant data from company databases. The chatbot should enable users to efficiently locate and extract specific data, answer queries, and generate reports.\\n\\n**Requirements Analysis:**\\n\\n1. **User Profile and Authentication**\\n\\t* Provide an option for users to log in using their Discord credentials or create a new account.\\n\\t* Implement role-based access control (RBAC) to ensure only authorized personnel have access to sensitive data.\\n2. **Data Retrieval and Filtering**\\n\\t* Integrate with company databases to retrieve relevant data based on user queries.\\n\\t* Allow users to filter data by various criteria, such as date ranges, data types, and fields of interest.\\n3. **Query Language and Syntax**\\n\\t* Develop a simple query language (e.g., SQL-like syntax) that allows users to formulate specific queries.\\n\\t* Provide auto-suggestions or suggestions based on commonly used queries to facilitate user input.\\n4. **Data Visualization and Report Generation**\\n\\t* Generate visualizations (e.g., charts, tables, graphs) to help users understand and present data insights.\\n\\t* Offer options for customizing report templates and formatting.\\n5. **Error Handling and Feedback Mechanism**\\n\\t* Implement a feedback loop to notify users of errors or inconsistencies in their queries.\\n\\t* Provide clear instructions and suggestions for resolving query-related issues.\\n6. **Security and Data Encryption**\\n\\t* Ensure all data transmitted between the chatbot and company databases is encrypted (e.g., SSL/TLS).\\n\\t* Store sensitive data securely, with access controls and audit logs to monitor user activity.\\n7. **User Interface and Experience**\\n\\t* Design an intuitive interface that accommodates users\\' varied preferences and expertise levels.\\n\\t* Incorporate clear instructions, tooltips, and help resources for users to navigate the chatbot\\'s features.\\n8. **Scalability and Performance**\\n\\t* Optimize the chatbot\\'s performance for large datasets and high user traffic.\\n\\t* Regularly update software components to maintain system reliability and security.\\n\\nPlease expand on these requirements to include more specific details and technical specifications, such as:\\n\\n* Database management systems (e.g., relational databases like MySQL, PostgreSQL; NoSQL databases like MongoDB)\\n* Frontend technologies (e.g., web frameworks', 'stream': False, 'options': {'temperature': 1.0, 'num_ctx': 500, 'num_predict': 1000}}\n",
      "\n",
      "Final Requirement Analysis: Here is an expanded version of the requirements with more specific details and technical specifications:\n",
      "\n",
      "**I. Requirements**\n",
      "\n",
      "1. **Database Management Systems**\n",
      "\t* Use a combination of relational and NoSQL databases to handle large datasets.\n",
      "\t* Utilize MySQL or PostgreSQL for relational database management, depending on the dataset's structure and complexity.\n",
      "\t* Implement MongoDB or Cassandra as the primary NoSQL database for handling large-scale data storage and retrieval.\n",
      "2. **Frontend Technologies**\n",
      "\t* Develop the chatbot's frontend using a modern web framework such as React, Angular, or Vue.js, to ensure responsiveness and scalability.\n",
      "\t* Leverage a JavaScript library like Material-UI or Bootstrap to create an intuitive and visually appealing interface.\n",
      "3. **Backend Frameworks**\n",
      "\t* Use Node.js with Express.js as the primary backend framework for handling API requests and interactions between the frontend and databases.\n",
      "\t* Utilize Next.js or Gatsby.js for server-side rendering and static site generation to improve performance and SEO.\n",
      "\n",
      "**II. Technical Specifications**\n",
      "\n",
      "1. **Database Schema Design**\n",
      "\t* Define a robust database schema using SQL to ensure efficient data retrieval, storage, and indexing.\n",
      "\t* Create separate tables for users, data sources, queries, and results to maintain data integrity and scalability.\n",
      "2. **API Endpoints**\n",
      "\t* Develop RESTful API endpoints to handle various requests from the frontend, including user authentication, query execution, and result retrieval.\n",
      "\t* Utilize API Gateway or AWS Lambda to route incoming requests to relevant endpoints, ensuring scalability and performance.\n",
      "\n",
      "**III. Database Schema Design**\n",
      "\n",
      "1. **Users Table**\n",
      "\t* Create a users table with columns for username, password (hashed), email, name, and profile picture.\n",
      "\t* Use an efficient data type such as varchar(255) or text to store the profile picture.\n",
      "2. **Data Sources Table**\n",
      "\t* Define a separate table for storing data sources, including column names, formats, and metadata.\n",
      "\t* Utilize a composite key (source_id, dataset_id) to ensure efficient join operations.\n",
      "\n",
      "**IV. Frontend Features**\n",
      "\n",
      "1. **User Interface Design**\n",
      "\t* Develop an intuitive interface using Material-UI or Bootstrap, with clear typography, color schemes, and responsive design.\n",
      "2. **Query Builder**\n",
      "\t* Create a query builder feature that allows users to construct queries using pre-defined fields, filters, and operators.\n",
      "3. **Data Display**\n",
      "\t* Utilize a library such as Chart.js or D3.js to render data visualizations in real-time.\n",
      "\n",
      "**V. Backend Security**\n",
      "\n",
      "1. **Authentication**\n",
      "\t* Implement user authentication using OAuth 2.0 or JWT tokens, ensuring secure access control and API protection.\n",
      "2. **Authorization**\n",
      "\t* Use role-based access control (RBAC) to restrict access to certain resources based on user roles.\n",
      "3. **Error Handling**\n",
      "\t* Develop a robust error handling mechanism that logs and displays error messages in a clear format.\n",
      "\n",
      "**VI. Deployment**\n",
      "\n",
      "1. **Cloud Services**\n",
      "\t* Utilize cloud services such as AWS or Google Cloud to host the application, ensuring scalability, performance, and reliability.\n",
      "2. **Containerization**\n",
      "\t* Employ containerization techniques using Docker or Kubernetes to ensure efficient deployment and management of microservices.\n",
      "3. **Monitoring and Logging**\n",
      "\t* Set up monitoring tools like Prometheus, Grafana, or ELK Stack to track application performance, latency, and errors.\n",
      "\n",
      "**VII. Testing**\n",
      "\n",
      "1. **Unit Testing**\n",
      "\t* Write comprehensive unit tests using frameworks such as Jest, Pytest, or Mocha to ensure individual component functionality.\n",
      "2. **Integration Testing**\n",
      "\t* Conduct integration testing using tools like Cypress or Postman to verify interactions between components.\n",
      "3. **UI Testing**\n",
      "\t* Use UI testing frameworks such as Enzyme or Cypress to validate user interface behavior and responsiveness.\n",
      "\n",
      "This guide provides a comprehensive framework for building, deploying, and maintaining a scalable web application.\n",
      "Final processing time: 13.696s\n"
     ]
    }
   ],
   "source": [
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) User's Initial Request for Requirement Analysis\n",
    "USER_REQUEST = \"\"\"\n",
    "Generate a prompt to generate requirement analysis for a Discord chatbot that:\n",
    "- Enables data scientists and business analysts to efficiently locate and extract relevant data from company databases to answer specific queries and generate reports.\n",
    "\"\"\"\n",
    "\n",
    "#### (2) First-Level Automation: Meta Prompting (Generating a well-structured prompt)\n",
    "meta_prompt = f\"\"\"\n",
    "You are an expert in prompt engineering. Given the following user request, generate a well-structured and detailed prompt that an AI model can use to generate a complete requirement analysis.\n",
    "\n",
    "User Request:\n",
    "{USER_REQUEST}\n",
    "\"\"\"\n",
    "\n",
    "# Send the first request to generate a structured prompt\n",
    "meta_payload = create_payload(\n",
    "    target=\"ollama\",\n",
    "    model=\"llama3.2:latest\",\n",
    "    prompt=meta_prompt,\n",
    "    temperature=1.0,\n",
    "    num_ctx=500,\n",
    "    num_predict=500,\n",
    ")\n",
    "\n",
    "meta_time, generated_prompt = model_req(payload=meta_payload)\n",
    "print(\"Generated AI Prompt for Requirement Analysis:\", generated_prompt)\n",
    "if meta_time:\n",
    "    print(f\"Meta prompting processing time: {meta_time}s\")\n",
    "\n",
    "#### (3) Second-Level Automation: Use the Generated Prompt for Final Requirement Analysis\n",
    "final_payload = create_payload(\n",
    "    target=\"ollama\",\n",
    "    model=\"llama3.2:latest\",\n",
    "    prompt=generated_prompt,  # Use the structured prompt\n",
    "    temperature=1.0,\n",
    "    num_ctx=500,\n",
    "    num_predict=1000,\n",
    ")\n",
    "\n",
    "# Send the final request to generate the requirement analysis\n",
    "final_time, final_response = model_req(payload=final_payload)\n",
    "print(\"\\nFinal Requirement Analysis:\", final_response)\n",
    "if final_time:\n",
    "    print(f\"Final processing time: {final_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
